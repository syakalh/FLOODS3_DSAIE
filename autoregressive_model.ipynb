{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoregressive CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Useful libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import copy\n",
    "import pickle\n",
    "from urllib.request import urlretrieve\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import random_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from matplotlib.colors import TwoSlopeNorm\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "# PyTorch Lightning\n",
    "try:\n",
    "    import pytorch_lightning as pl\n",
    "except ModuleNotFoundError: # Google Colab does not have PyTorch Lightning installed by default. Hence, we do it here if necessary\n",
    "    !pip install --quiet pytorch-lightning>=1.4\n",
    "    import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint\n",
    "\n",
    "from cycler import cycler\n",
    "import seaborn as sns\n",
    "\n",
    "# Set the color scheme\n",
    "sns.set_theme()\n",
    "colors = ['#0076C2', '#EC6842', '#A50034', '#009B77', '#FFB81C', '#E03C31', '#6CC24A', '#EF60A3', '#0C2340', '#00B8C8', '#6F1D77']\n",
    "plt.rcParams['axes.prop_cycle'] = cycler(color=colors)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "length_train_val_data = 80\n",
    "\n",
    "DEM_train_val = torch.zeros((length_train_val_data, 64, 64))\n",
    "\n",
    "training = 0.8\n",
    "validation = 0.2\n",
    "\n",
    "for k in range(length_train_val_data):\n",
    "  DEM = np.genfromtxt(f'raw_datasets/DEM/DEM_{k+1}.txt')\n",
    "  DEM_t = torch.as_tensor(DEM, dtype=torch.float32)\n",
    "\n",
    "\n",
    "  for x, y, elevation in zip(DEM_t[:,0], DEM_t[:,1], DEM_t[:,2]):\n",
    "      # Convert coordinates to indices in the 64x64 tensor\n",
    "      i = int((y - 50) / 100)\n",
    "      j = int((x - 50) / 100)\n",
    "\n",
    "      # Assign the elevation value to the corresponding position in the tensor\n",
    "      DEM_train_val[k, i, j] = elevation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "VX_train_val = torch.zeros((length_train_val_data, 97, 64, 64))\n",
    "\n",
    "training = 0.8\n",
    "validation = 0.2\n",
    "\n",
    "for k in range(length_train_val_data):\n",
    "  VX = np.genfromtxt(f'raw_datasets/VX/VX_{k+1}.txt')\n",
    "  VX_t = torch.as_tensor(VX, dtype=torch.float32)\n",
    "\n",
    "\n",
    "  for x, y, elevation in zip(VX_t[:,0], VX_t[:,1], VX_t[:,2]):\n",
    "      # Convert coordinates to indices in the 64x64 tensor\n",
    "      i = int((y - 50) / 100)\n",
    "      j = int((x - 50) / 100)\n",
    "\n",
    "      # Assign the elevation value to the corresponding position in the tensor\n",
    "      VX_train_val[k, :, i, j] = elevation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "vDxq0tOGHtym"
   },
   "outputs": [],
   "source": [
    "VY_train_val = torch.zeros((length_train_val_data, 97, 64, 64))\n",
    "\n",
    "training = 0.8\n",
    "validation = 0.2\n",
    "\n",
    "for k in range(length_train_val_data):\n",
    "  VY = np.genfromtxt(f'raw_datasets/VY/VY_{k+1}.txt')\n",
    "  VY_t = torch.as_tensor(VY, dtype=torch.float32)\n",
    "\n",
    "\n",
    "  for x, y, elevation in zip(VY_t[:,0], VY_t[:,1], VY_t[:,2]):\n",
    "      # Convert coordinates to indices in the 64x64 tensor\n",
    "      i = int((y - 50) / 100)\n",
    "      j = int((x - 50) / 100)\n",
    "\n",
    "      # Assign the elevation value to the corresponding position in the tensor\n",
    "      VY_train_val[k, i, j] = elevation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "F_F6k4QGIyOe"
   },
   "outputs": [],
   "source": [
    "WD_train_val = torch.zeros((length_train_val_data, 97, 64, 64))\n",
    "\n",
    "training = 0.8\n",
    "validation = 0.2\n",
    "\n",
    "for i in range(length_train_val_data):\n",
    "  WD = np.genfromtxt(f'raw_datasets/WD/WD_{i+1}.txt')\n",
    "  WD_t = torch.as_tensor(WD, dtype=torch.float32)\n",
    "\n",
    "  for k in range(97):\n",
    "    wd = WD_t[k].reshape((64,64))\n",
    "    wd = torch.as_tensor(wd)\n",
    "\n",
    "    WD_train_val[i, k] = wd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "ao8LvyT-W1Hd"
   },
   "outputs": [],
   "source": [
    "length_train_val_data = 80\n",
    "\n",
    "# Assuming you have a tensor 'WD_train_val' with shape (length_train_val_data, 97, 64, 64)\n",
    "WD_train_val_reshaped = torch.zeros((length_train_val_data, 24, 64, 64))\n",
    "\n",
    "for i in range(length_train_val_data):\n",
    "    WD = np.genfromtxt(f'raw_datasets/WD/WD_{i+1}.txt')\n",
    "    WD_t = torch.as_tensor(WD, dtype=torch.float32)\n",
    "\n",
    "    for j in range(24):\n",
    "        # Extract a 2-hour interval from the original 97 time points\n",
    "        start_index = j * 4  # Each 2-hour interval has 4 time points (assuming 30 minutes intervals)\n",
    "        end_index = (j + 1) * 4\n",
    "        wd = WD_t[j].reshape((64,64))\n",
    "        wd = torch.as_tensor(wd)\n",
    "        # Average or concatenate the data over the 2-hour interval, depending on your requirement\n",
    "        wd_interval = torch.mean(wd[start_index:end_index], dim=0)  # You can use other aggregation functions if needed\n",
    "\n",
    "        WD_train_val_reshaped[i, j] = wd_interval\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8x3Lkd5qUcfw"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "e4YnB_9BDg42"
   },
   "outputs": [],
   "source": [
    "input_train_dataset = torch.stack((DEM_train_val, WD_train_val[:,0])).permute(1, 0, 2, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "k1Gf6V_gJ7Cg"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([80, 2, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "print(input_train_dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "XjJHV1cmzchs"
   },
   "outputs": [],
   "source": [
    "output_train_dataset = WD_train_val[:,1:97]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "M0CPWl0LzpPW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([80, 96, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "print(output_train_dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "p19UN3cQYijy"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(80, 2)\n",
      "torch.Size([96, 64, 64])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bkrans/miniconda3/envs/dsaie/lib/python3.9/site-packages/numpy/core/fromnumeric.py:2009: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  result = asarray(a).shape\n",
      "/Users/bkrans/miniconda3/envs/dsaie/lib/python3.9/site-packages/numpy/core/fromnumeric.py:2009: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  result = asarray(a).shape\n"
     ]
    }
   ],
   "source": [
    "train_dataset = []\n",
    "\n",
    "# Iterate through the samples\n",
    "for i in range(input_train_dataset.size(0)):\n",
    "    # Get the tensors for the current sample\n",
    "    sample_tensor1 = input_train_dataset[i]  # Shape: [2, 64, 64]\n",
    "    sample_tensor2 = output_train_dataset[i]  # Shape: [96, 64, 64]\n",
    "\n",
    "    # Append the tensors to the train_dataset list\n",
    "    train_dataset.append([sample_tensor1, sample_tensor2])\n",
    "\n",
    "# Convert the list to a PyTorch tensor\n",
    "# train_dataset = torch.stack([torch.stack(sample) for sample in train_dataset])\n",
    "\n",
    "print(np.shape(train_dataset))\n",
    "\n",
    "print(np.shape(train_dataset[0][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "AOpAjEKIZNQF"
   },
   "outputs": [],
   "source": [
    "# print(train_dataset.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YQI5pOJYfO8N"
   },
   "source": [
    "Test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "xeBP6f06fOO3"
   },
   "outputs": [],
   "source": [
    "length_test_data = 20\n",
    "\n",
    "DEM_test = torch.zeros((length_test_data, 64, 64))\n",
    "\n",
    "training = 0.8\n",
    "validation = 0.2\n",
    "\n",
    "for k in range(length_test_data):\n",
    "  DEM = np.genfromtxt(f'raw_datasets/DEM/DEM_{k+500}.txt')\n",
    "\n",
    "  DEM_t = torch.as_tensor(DEM, dtype=torch.float32)\n",
    "\n",
    "\n",
    "  for x, y, elevation in zip(DEM_t[:,0], DEM_t[:,1], DEM_t[:,2]):\n",
    "      # Convert coordinates to indices in the 64x64 tensor\n",
    "      i = int((y - 50) / 100)\n",
    "      j = int((x - 50) / 100)\n",
    "\n",
    "      # Assign the elevation value to the corresponding position in the tensor\n",
    "      DEM_test[k, i, j] = elevation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "gNyk4XtRf6zC"
   },
   "outputs": [],
   "source": [
    "WD_test = torch.zeros((length_test_data, 97, 64, 64))\n",
    "\n",
    "training = 0.8\n",
    "validation = 0.2\n",
    "\n",
    "for i in range(length_test_data):\n",
    "  WD = np.genfromtxt(f'raw_datasets/WD/WD_{i+500}.txt')\n",
    "  WD_t = torch.as_tensor(WD, dtype=torch.float32)\n",
    "\n",
    "  for k in range(97):\n",
    "    wd = WD_t[k].reshape((64,64))\n",
    "    wd = torch.as_tensor(wd)\n",
    "\n",
    "    WD_test[i, k] = wd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "Ckiqw-dxglcd"
   },
   "outputs": [],
   "source": [
    "length_test_data = 20\n",
    "\n",
    "# Assuming you have a tensor 'WD_train_val' with shape (length_train_val_data, 97, 64, 64)\n",
    "WD_test_reshaped = torch.zeros((length_test_data, 24, 64, 64))\n",
    "\n",
    "for i in range(length_test_data):\n",
    "    WD = np.genfromtxt(f'raw_datasets/WD/WD_{i+500}.txt')\n",
    "    WD_t = torch.as_tensor(WD, dtype=torch.float32)\n",
    "\n",
    "    for j in range(24):\n",
    "        # Extract a 2-hour interval from the original 97 time points\n",
    "        start_index = j * 4  # Each 2-hour interval has 4 time points (assuming 30 minutes intervals)\n",
    "        end_index = (j + 1) * 4\n",
    "        wd = WD_t[j].reshape((64,64))\n",
    "        wd = torch.as_tensor(wd)\n",
    "        # Average or concatenate the data over the 2-hour interval, depending on your requirement\n",
    "        wd_interval = torch.mean(wd[start_index:end_index], dim=0)  # You can use other aggregation functions if needed\n",
    "\n",
    "        WD_test_reshaped[i, j] = wd_interval\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "b98DCTmRiWCA"
   },
   "outputs": [],
   "source": [
    "input_test_dataset = torch.stack((DEM_test, WD_test[:,0])).permute(1, 0, 2, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "Z1LeLlMDiWRu"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 2, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "print(input_test_dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "FRyd5zqqiWbb"
   },
   "outputs": [],
   "source": [
    "output_test_dataset = WD_test[:,1:97]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "pN9QGSG3i19N"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 96, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "print(output_test_dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "P3rbS94Yi7Ki"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 2)\n",
      "torch.Size([96, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "test_dataset = []\n",
    "\n",
    "# Iterate through the samples\n",
    "for i in range(input_test_dataset.size(0)):\n",
    "    # Get the tensors for the current sample\n",
    "    sample_tensor1 = input_test_dataset[i]  # Shape: [2, 64, 64]\n",
    "    sample_tensor2 = output_test_dataset[i]  # Shape: [96, 64, 64]\n",
    "\n",
    "    # Append the tensors to the train_dataset list\n",
    "    test_dataset.append([sample_tensor1, sample_tensor2])\n",
    "\n",
    "# Convert the list to a PyTorch tensor\n",
    "# train_dataset = torch.stack([torch.stack(sample) for sample in train_dataset])\n",
    "\n",
    "print(np.shape(test_dataset))\n",
    "\n",
    "print(np.shape(test_dataset[0][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedConvolution(nn.Module):\n",
    "\n",
    "    def __init__(self, c_in, c_out, mask, **kwargs):\n",
    "        \"\"\"\n",
    "        Implements a convolution with mask applied on its weights.\n",
    "        Inputs:\n",
    "            c_in - Number of input channels\n",
    "            c_out - Number of output channels\n",
    "            mask - Tensor of shape [kernel_size_H, kernel_size_W] with 0s where\n",
    "                   the convolution should be masked, and 1s otherwise.\n",
    "            kwargs - Additional arguments for the convolution\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # For simplicity: calculate padding automatically\n",
    "        kernel_size = (mask.shape[0], mask.shape[1])\n",
    "        dilation = 1 if \"dilation\" not in kwargs else kwargs[\"dilation\"]\n",
    "        padding = tuple([dilation*(kernel_size[i]-1)//2 for i in range(2)])\n",
    "        # Actual convolution\n",
    "        self.conv = nn.Conv2d(c_in, c_out, kernel_size, padding=padding, **kwargs)\n",
    "\n",
    "        # Mask as buffer => it is no parameter but still a tensor of the module\n",
    "        # (must be moved with the devices)\n",
    "        self.register_buffer('mask', mask[None,None])\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.conv.weight.data *= self.mask # Ensures zero's at masked positions\n",
    "        return self.conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VerticalStackConvolution(MaskedConvolution):\n",
    "\n",
    "    def __init__(self, c_in, c_out, kernel_size=3, mask_center=False, **kwargs):\n",
    "        # Mask out all pixels below. For efficiency, we could also reduce the kernel\n",
    "        # size in height, but for simplicity, we stick with masking here.\n",
    "        mask = torch.ones(kernel_size, kernel_size)\n",
    "        mask[kernel_size//2+1:,:] = 0\n",
    "\n",
    "        # For the very first convolution, we will also mask the center row\n",
    "        if mask_center:\n",
    "            mask[kernel_size//2,:] = 0\n",
    "\n",
    "        super().__init__(c_in, c_out, mask, **kwargs)\n",
    "\n",
    "class HorizontalStackConvolution(MaskedConvolution):\n",
    "\n",
    "    def __init__(self, c_in, c_out, kernel_size=3, mask_center=False, **kwargs):\n",
    "        # Mask out all pixels on the left. Note that our kernel has a size of 1\n",
    "        # in height because we only look at the pixel in the same row.\n",
    "        mask = torch.ones(1,kernel_size)\n",
    "        mask[0,kernel_size//2+1:] = 0\n",
    "\n",
    "        # For the very first convolution, we will also mask the center pixel\n",
    "        if mask_center:\n",
    "            mask[0,kernel_size//2] = 0\n",
    "\n",
    "        super().__init__(c_in, c_out, mask, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GatedMaskedConv(nn.Module):\n",
    "\n",
    "    def __init__(self, c_in, **kwargs):\n",
    "        \"\"\"\n",
    "        Gated Convolution block implemented the computation graph shown above.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.conv_vert = VerticalStackConvolution(c_in, c_out=2*c_in, **kwargs)\n",
    "        self.conv_horiz = HorizontalStackConvolution(c_in, c_out=2*c_in, **kwargs)\n",
    "        self.conv_vert_to_horiz = nn.Conv2d(2*c_in, 2*c_in, kernel_size=1, padding=0)\n",
    "        self.conv_horiz_1x1 = nn.Conv2d(c_in, c_in, kernel_size=1, padding=0)\n",
    "\n",
    "    def forward(self, v_stack, h_stack):\n",
    "        # Vertical stack (left)\n",
    "        v_stack_feat = self.conv_vert(v_stack)\n",
    "        v_val, v_gate = v_stack_feat.chunk(2, dim=1)\n",
    "        v_stack_out = torch.tanh(v_val) * torch.sigmoid(v_gate)\n",
    "\n",
    "        # Horizontal stack (right)\n",
    "        h_stack_feat = self.conv_horiz(h_stack)\n",
    "        h_stack_feat = h_stack_feat + self.conv_vert_to_horiz(v_stack_feat)\n",
    "        h_val, h_gate = h_stack_feat.chunk(2, dim=1)\n",
    "        h_stack_feat = torch.tanh(h_val) * torch.sigmoid(h_gate)\n",
    "        h_stack_out = self.conv_horiz_1x1(h_stack_feat)\n",
    "        h_stack_out = h_stack_out + h_stack\n",
    "\n",
    "        return v_stack_out, h_stack_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Model\n",
    "\n",
    "class PixelCNN(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, c_in, c_hidden):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        # Initial convolutions skipping the center pixel\n",
    "        self.conv_vstack = VerticalStackConvolution(c_in, c_hidden, mask_center=True)\n",
    "        self.conv_hstack = HorizontalStackConvolution(c_in, c_hidden, mask_center=True)\n",
    "        # Convolution block of PixelCNN. We use dilation instead of downscaling\n",
    "        self.conv_layers = nn.ModuleList([\n",
    "            GatedMaskedConv(c_hidden),\n",
    "            GatedMaskedConv(c_hidden, dilation=2),\n",
    "            GatedMaskedConv(c_hidden),\n",
    "            GatedMaskedConv(c_hidden, dilation=4),\n",
    "            GatedMaskedConv(c_hidden),\n",
    "            GatedMaskedConv(c_hidden, dilation=2),\n",
    "            GatedMaskedConv(c_hidden)\n",
    "        ])\n",
    "        # Output classification convolution (1x1)\n",
    "        self.conv_out = nn.Conv2d(c_hidden, c_in * 256, kernel_size=1, padding=0)\n",
    "\n",
    "        self.example_input_array = train_set[0][0][None]\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward image through model and return logits for each pixel.\n",
    "        Inputs:\n",
    "            x - Image tensor with integer values between 0 and 255.\n",
    "        \"\"\"\n",
    "        # Scale input from 0 to 255 back to -1 to 1\n",
    "        x = (x.float() / 255.0) * 2 - 1\n",
    "\n",
    "        # Initial convolutions\n",
    "        v_stack = self.conv_vstack(x)\n",
    "        h_stack = self.conv_hstack(x)\n",
    "        # Gated Convolutions\n",
    "        for layer in self.conv_layers:\n",
    "            v_stack, h_stack = layer(v_stack, h_stack)\n",
    "        # 1x1 classification convolution\n",
    "        # Apply ELU before 1x1 convolution for non-linearity on residual connection\n",
    "        out = self.conv_out(F.elu(h_stack))\n",
    "\n",
    "        # Output dimensions: [Batch, Classes, Channels, Height, Width]\n",
    "        out = out.reshape(out.shape[0], 256, out.shape[1]//256, out.shape[2], out.shape[3])\n",
    "        return out\n",
    "\n",
    "    def calc_likelihood(self, x):\n",
    "        # Forward pass with bpd likelihood calculation\n",
    "        pred = self.forward(x)\n",
    "        nll = F.cross_entropy(pred, x, reduction='none')\n",
    "        bpd = nll.mean(dim=[1,2,3]) * np.log2(np.exp(1))\n",
    "        return bpd.mean()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def sample(self, img_shape, img=None):\n",
    "        \"\"\"\n",
    "        Sampling function for the autoregressive model.\n",
    "        Inputs:\n",
    "            img_shape - Shape of the image to generate (B,C,H,W)\n",
    "            img (optional) - If given, this tensor will be used as\n",
    "                             a starting image. The pixels to fill\n",
    "                             should be -1 in the input tensor.\n",
    "        \"\"\"\n",
    "        # Create empty image\n",
    "        if img is None:\n",
    "            img = torch.zeros(img_shape, dtype=torch.long).to(device) - 1\n",
    "        # Generation loop\n",
    "        for h in tqdm(range(img_shape[2]), leave=False):\n",
    "            for w in range(img_shape[3]):\n",
    "                for c in range(img_shape[1]):\n",
    "                    # Skip if not to be filled (-1)\n",
    "                    if (img[:,c,h,w] != -1).all().item():\n",
    "                        continue\n",
    "                    # For efficiency, we only have to input the upper part of the image\n",
    "                    # as all other parts will be skipped by the masked convolutions anyways\n",
    "                    pred = self.forward(img[:,:,:h+1,:])\n",
    "                    probs = F.softmax(pred[:,:,c,h,w], dim=-1)\n",
    "                    img[:,c,h,w] = torch.multinomial(probs, num_samples=1).squeeze(dim=-1)\n",
    "        return img\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.Adam(self.parameters(), lr=1e-3)\n",
    "        scheduler = optim.lr_scheduler.StepLR(optimizer, 1, gamma=0.99)\n",
    "        return [optimizer], [scheduler]\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss = self.calc_likelihood(batch[0])\n",
    "        self.log('train_bpd', loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss = self.calc_likelihood(batch[0])\n",
    "        self.log('val_bpd', loss)\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        loss = self.calc_likelihood(batch[0])\n",
    "        self.log('test_bpd', loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model multistep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_multistep(model, test_loader, criterion, device, T, H):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    test_loss = 0\n",
    "\n",
    "    all_predictions = []  # List to store all batch predictions\n",
    "    all_targets = []      # List to store all batch targets\n",
    "\n",
    "    with torch.no_grad():  # No need to track gradients during evaluation\n",
    "        for initial_inputs, initial_targets in test_loader:\n",
    "            step_inputs = initial_inputs.to(device)\n",
    "            targets = initial_targets.to(device)\n",
    "\n",
    "            # Holds predictions for comparison with targets\n",
    "            predictions = []\n",
    "\n",
    "            # Iterate for H steps\n",
    "            for h in range(H):\n",
    "                outputs = model(step_inputs)\n",
    "                predictions.append(outputs)\n",
    "\n",
    "                # Reshape or expand outputs to be 3D: [batch_size, 1, features]\n",
    "                next_input = outputs.unsqueeze(-1).squeeze(1)  # Adjust the dimensions as necessary\n",
    "\n",
    "                # Update step_inputs by sliding the window: remove the oldest input and add the new output\n",
    "                # Ensure that step_inputs and next_input are correctly shaped for concatenation\n",
    "                step_inputs = torch.cat((step_inputs[:, 1:], next_input), dim=1)\n",
    "\n",
    "            # Concatenate predictions and calculate loss against the entire target sequence\n",
    "            batch_predictions  = torch.stack(predictions, dim=1).squeeze(-1)  # Squeeze the last dimension\n",
    "\n",
    "            loss = criterion(batch_predictions , targets)\n",
    "            test_loss += loss.item()\n",
    "\n",
    "            # Store batch predictions and targets\n",
    "            all_predictions.append(batch_predictions.cpu())\n",
    "            all_targets.append(targets.cpu())\n",
    "\n",
    "    # Concatenate all batch predictions and targets into tensors\n",
    "    all_predictions = torch.cat(all_predictions, dim=0)\n",
    "    all_targets = torch.cat(all_targets, dim=0)\n",
    "\n",
    "    avg_test_loss = test_loss / len(test_loader)\n",
    "    return avg_test_loss, all_predictions, all_targets"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
